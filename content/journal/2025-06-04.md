---
title: 2025-06-04 Journal
date: 2025-06-04 19:46:29 -05:00
# draft: true
tags:
  - journal
type: journal
foam_template:
  description: Daily journal notes
---

## Reflecting on my crazy ideas

It all started when YouTube recommended me [this video](https://www.youtube.com/watch?v=IroPQ150F6c)
on [[data-oriented design]] by [[Zig]] creator Andrew Kelley that sent me down
the rabbit hole.

Finding the video was timely because some of my coworkers and
I had recently been talking about a few interesting encounters we had a few
years ago with some of our numerical software running in our computer cluster
at work.  Our cluster has many compute nodes in the pool amalgamated from many
different generations of hardware, so submitting distributed parallelized jobs
was running our [[Fortran]] code on many different CPUs.  Because we use the
Intel Fortran Compiler, it is able to leverage Intel's knowledge about their own
hardware to provide additional compiler optimizations beyond what something like
`gfortran` might offer...depending on the instruction sets available on each CPU.

It seemed that, after drag racing the same code on various CPUs, the line in the
sand appeared sometime after Ivy Bridge, which was a clue that the compiler must
be trying to use [[AVX2]] [[SIMD]] instructions.
It is unclear if those instructions were either causing precision loss or that
it was the regular, scalar non-SIMD instructions were the less precise answer.
In either case, for particularly numerically sensitive cases in our code, it was
changing the answers in a significant way that led us to conclude that it's
probably better to just be consistent rather than track down which one was
actually giving the better answer.

But I was never happy with that conclusion, to be honest, because I wanted to be
able to have more control over how the code was being optimized.  Sure, you can
control things with compiler flags and the like, but at that level, it would
apply to the entire file.  With a low-level language like the
[[C programming language]], the programmer can have much more granular control
over which lines are optimized and which are not than Fortran can.
And, of course, C allows you to always fall back to just brute-forcing
adding your own [[assembly]] code.

The main selling point of using Fortran is the fact that every Fortran compiler
out of the box gives you better code optimizations without requiring the
programmer to have to worry about such things as per-line optimization control.
It probably feels either some combination of blasphemous or at least frightening
to most Fortran developers to even want to try to intervene in the code at that
level because, at the end of the day, most Fortran developers are not computer
scientists, and that's the point.  They hand over the reins to people much
smarter than them that figured all this stuff out long ago so they can just
type equations and let the compiler pick the best way to conver it to machine
instructions.  Generally, it gives you the right and objectively right answer.

But when suddenly such Fortran codes must enter the thunderdome of grappling
with code running on different architectures simultaneously, this is should be
the domain of a more system-oriented language to help reconcile such
differences and try to ensure you can reliably get answers via the most
performant code with the minimized numerical differences where possible.
Consistency should be king, and leaving these kinds of SIMD-level decisions
up to the compiler in places where the engineer knows it is particularly
numerically sensitive is perhaps not wise.

The real problem, however, is the kind of math my team needs to do.  Many of
our calculations revolve around the handling of [[sparse matrices]], for which
there are few existing libraries that can reliably handle such data.  While
time-tested libraries like [[BLAS]] have existed for a long time to handle
dense matrices, allowing developers to just call those pre-compiled libraries
tuned to give you the best, closest answer reliably on any platform,
such counterparts do not practically exist for sparse matrices.  This means that
developers tend to just share source code for methods that handle sparse
matrices, but their compilation is then left to the whims of the developer
inheriting such codes.  And this seems to be the original sin in this case,
where the sparse matrix operations can *sometimes* be optimized as SIMD in rare
cases, but the Fortran compiler can leave things to chance.

What I set out to do after this was develop a custom BLAS-like
fundamental library for sparse matrix operations where I could control the
compiler optimization at a more granular level given *a priori* knowledge of
where things might be sensitive and make appropriate choices accordingly.

It's an idea I'd tossed around in my head for a while, but hadn't given serious
thought because I didn't really know enough about SIMD myself to be able to
have that level of control.  Sure, while I could go read up on that, one of the
things I wasn't clear about was if how the data was stored could affect
the ability to invoke SIMD manually rather than fall back to scalar
operations.  And given the potential size of the data, the performance hit of
scalar operations could be significant.  I figured I would one day in my free
time figure out how to implement sparse data structures, but it was always on
the back burner.

The thing I did not appreciate until watching Andrew Kelley's video is the
criticality of the [[cache line]].  Data-oriented design is about designing
data structures that minimize cache misses so you can more efficiently loop over
the data and reduce run times or even power usage.  You want to minimize random
access where possible to large data structures, and you want to organize your
code to ensure you don't do two operations back-to-back that might result in
*evicting* the cache line.  While I think many of the techniques employed by
adherents to data-oriented design are focused on regular scalar looping
operations, I have to believe these ideas must extend to SIMD interactions and
"priming the pump" for SIMD registers with effective use of cache and
prefetching.  But this is difficult to manage with something like sparse matrix
operations because most common techniques for storing or handling them involve
random reads from main memory rather than leveraging CPU caches.

And so, upon watching that video, I decided to embark on a journey of studying
BLAS implementations to find or develop replacement functions that efficiently
operate on sparse matrices.  And the key to this would be developing a new
technique for storing sparse matrices perhaps informed by data-oriented design.

I came across [this paper](https://www.osti.gov/servlets/purl/1367134) from
some folks at Sandia that, in short, suggest that one can get some performance
improvement in sparse multiplication operations by "pre-multiplying" their
sparsity patterns, represented as bitmasks so that simple bitwise operations
could be used to determine the result.  Then, once the sparsity pattern of the
result is known, the calculations can be reduced to only the remaining handful
of non-zero elements.  This seems like it might be helpful.

I also began to engage [[ChatGPT]] on this topic to help me develop my new
sparse data structure.  I started by looking into existing formulations of
sparse matrices on Wikipedia, and quickly zeroed in on the three most common
representations (note that `M` is number of rows, `N` columns, and `NNZ` is the
number of non-zero elements):

* **COO**: Coordinate form.  Values are tracked functionally as triples
  of `(row, col, value)`
* **CSR**: Compressed Sparse Rows.  Arrays for values and corresponding column
  index have length NNZ, while a row index array has length M + 1 pointing to
  index in values/columns of the first in that row.
* **CSC**: Compressed Sparse Columns.  Same as above, but the row/column
  functions above are swapped.

My first thought was that storing column and row separately if you know M and N
is wasting space because it could be collapsed into a single index; to recover
the original row/column index, you just need to do modulo/integer division.
Doing so could accelerate linear access like looping, but makes random access
much harder because you have to unpack every index searching from the beginning
somehow.

This is where the idea came about to have *bucketing*, where you do
something similar to CSR/CSC but instead of every row tracked in one of the
indices, you track some fixed number of rows or columns in a bucket.
For example, if the bucket fixed size is 128, then if you request row 130, you
first integer divide the given row by the bucket size (130//128 = 1) and then
check the bucket index to get the index of the values array that starts that
bucket.  For very large arrays, this allows fast-forwarding into the middle
of the matrix for random access without needing to search from the very
beginning of the value/index arrays while saving space in memory.  For linear
access such as multiplication operations, it retains the speed advantages of
having the packed data without complex structures.

I will continue this train of thought tomorrow perhaps.
